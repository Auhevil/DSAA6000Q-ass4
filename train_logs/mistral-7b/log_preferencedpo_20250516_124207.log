[2025-05-16 12:42:12,438] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 05-16 12:42:13 __init__.py:190] Automatically detected platform cuda.
[INFO|2025-05-16 12:42:15] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:20183
W0516 12:42:17.014000 4078036 site-packages/torch/distributed/run.py:793] 
W0516 12:42:17.014000 4078036 site-packages/torch/distributed/run.py:793] *****************************************
W0516 12:42:17.014000 4078036 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0516 12:42:17.014000 4078036 site-packages/torch/distributed/run.py:793] *****************************************
[2025-05-16 12:42:21,183] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-16 12:42:22,051] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-05-16 12:42:22] llamafactory.hparams.parser:383 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[rank1]:[W516 12:42:22.944702476 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[WARNING|2025-05-16 12:42:23] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-05-16 12:42:23] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,648 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,649 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,649 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,649 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,649 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,649 >> loading file chat_template.jinja
[INFO|configuration_utils.py:697] 2025-05-16 12:42:23,728 >> loading configuration file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:771] 2025-05-16 12:42:23,730 >> Model config MistralConfig {
  "_name_or_path": "/data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-05-16 12:42:23,730 >> loading file chat_template.jinja
[INFO|2025-05-16 12:42:23] llamafactory.data.template:143 >> Add pad token: </s>
[INFO|2025-05-16 12:42:23] llamafactory.data.loader:143 >> Loading dataset preference-dataset.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 50 examples [00:00, 2322.20 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/50 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  22%|██▏       | 11/50 [00:00<00:00, 105.76 examples/s]Converting format of dataset (num_proc=16):  82%|████████▏ | 41/50 [00:00<00:00, 216.49 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50/50 [00:00<00:00, 156.45 examples/s]
[rank0]:[W516 12:42:25.844103288 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/50 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 4/50 [00:00<00:02, 16.47 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 14/50 [00:00<00:00, 42.39 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 23/50 [00:00<00:00, 52.20 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 32/50 [00:00<00:00, 59.03 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 41/50 [00:00<00:00, 67.65 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50/50 [00:00<00:00, 72.68 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50/50 [00:00<00:00, 54.11 examples/s]
training example:
chosen_input_ids:
[1, 733, 16289, 28793, 12018, 1712, 23872, 684, 8222, 28792, 28748, 16289, 28793, 259, 28740, 28723, 4315, 949, 28742, 28707, 5939, 721, 1549, 938, 18518, 28804, 13, 13, 17098, 590, 28742, 267, 8526, 302, 272, 12733, 28808, 13, 13, 28750, 28723, 4315, 863, 272, 752, 492, 28717, 671, 5175, 272, 4713, 381, 28804, 13, 13, 17098, 400, 403, 17329, 297, 516, 1834, 28808, 13, 13, 28770, 28723, 1824, 511, 368, 1034, 264, 12863, 9135, 28804, 13, 13, 28741, 1135, 1031, 288, 9135, 28808, 1092, 3229, 28725, 456, 349, 776, 264, 13015, 28725, 304, 297, 1353, 1411, 28725, 368, 1023, 1484, 4431, 264, 12863, 9135, 442, 707, 6107, 8527, 28723, 2]
chosen_inputs:
<s> [INST] Write three jokes about animals[/INST]  1. Why don't elephants use computers?

Because they're afraid of the mouse!

2. Why did the scarecrow join the circus?

Because he was outstanding in his field!

3. What do you call a sleeping bear?

A slumbering bear! But remember, this is just a joke, and in real life, you should never approach a sleeping bear or any wild animal.</s>
chosen_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 259, 28740, 28723, 4315, 949, 28742, 28707, 5939, 721, 1549, 938, 18518, 28804, 13, 13, 17098, 590, 28742, 267, 8526, 302, 272, 12733, 28808, 13, 13, 28750, 28723, 4315, 863, 272, 752, 492, 28717, 671, 5175, 272, 4713, 381, 28804, 13, 13, 17098, 400, 403, 17329, 297, 516, 1834, 28808, 13, 13, 28770, 28723, 1824, 511, 368, 1034, 264, 12863, 9135, 28804, 13, 13, 28741, 1135, 1031, 288, 9135, 28808, 1092, 3229, 28725, 456, 349, 776, 264, 13015, 28725, 304, 297, 1353, 1411, 28725, 368, 1023, 1484, 4431, 264, 12863, 9135, 442, 707, 6107, 8527, 28723, 2]
chosen_labels:
 1. Why don't elephants use computers?

Because they're afraid of the mouse!

2. Why did the scarecrow join the circus?

Because he was outstanding in his field!

3. What do you call a sleeping bear?

A slumbering bear! But remember, this is just a joke, and in real life, you should never approach a sleeping bear or any wild animal.</s>
rejected_input_ids:
[1, 733, 16289, 28793, 12018, 1712, 23872, 684, 8222, 28792, 28748, 16289, 28793, 259, 28740, 28723, 4315, 949, 28742, 28707, 5939, 721, 1549, 9123, 297, 272, 461, 22082, 28804, 5518, 590, 28742, 267, 1528, 1179, 438, 378, 28808, 13, 13, 28750, 28723, 1824, 511, 368, 1034, 264, 12863, 9135, 28804, 330, 1135, 1031, 288, 9135, 28723, 1824, 511, 368, 1034, 264, 9135, 369, 28742, 28713, 3694, 15231, 28804, 330, 9135, 28733, 28724, 4289, 28724, 1811, 28808, 13, 13, 28770, 28723, 4315, 863, 272, 752, 492, 28717, 671, 5175, 272, 4713, 381, 28804, 5518, 400, 403, 17329, 297, 516, 1834, 28808, 1092, 264, 1326, 1499, 1988, 2267, 304, 6373, 713, 264, 8743, 2238, 28725, 579, 400, 14597, 304, 7298, 272, 1326, 1499, 1955, 3519, 28723, 325, 3260, 624, 28742, 28713, 264, 11495, 3579, 28733, 308, 416, 267, 354, 8527, 24883, 14280, 2]
rejected_inputs:
<s> [INST] Write three jokes about animals[/INST]  1. Why don't elephants hide in the jungle? Because they're really good at it!

2. What do you call a sleeping bear? A slumbering bear. What do you call a bear that's dead asleep? A bear-y sleepyhead!

3. Why did the scarecrow join the circus? Because he was outstanding in his field! But a monkey came along and offered him a banana, so he quit and joined the monkey business instead. (This one's a classic double-entendre for animal lovers!)</s>
rejected_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 259, 28740, 28723, 4315, 949, 28742, 28707, 5939, 721, 1549, 9123, 297, 272, 461, 22082, 28804, 5518, 590, 28742, 267, 1528, 1179, 438, 378, 28808, 13, 13, 28750, 28723, 1824, 511, 368, 1034, 264, 12863, 9135, 28804, 330, 1135, 1031, 288, 9135, 28723, 1824, 511, 368, 1034, 264, 9135, 369, 28742, 28713, 3694, 15231, 28804, 330, 9135, 28733, 28724, 4289, 28724, 1811, 28808, 13, 13, 28770, 28723, 4315, 863, 272, 752, 492, 28717, 671, 5175, 272, 4713, 381, 28804, 5518, 400, 403, 17329, 297, 516, 1834, 28808, 1092, 264, 1326, 1499, 1988, 2267, 304, 6373, 713, 264, 8743, 2238, 28725, 579, 400, 14597, 304, 7298, 272, 1326, 1499, 1955, 3519, 28723, 325, 3260, 624, 28742, 28713, 264, 11495, 3579, 28733, 308, 416, 267, 354, 8527, 24883, 14280, 2]
rejected_labels:
 1. Why don't elephants hide in the jungle? Because they're really good at it!

2. What do you call a sleeping bear? A slumbering bear. What do you call a bear that's dead asleep? A bear-y sleepyhead!

3. Why did the scarecrow join the circus? Because he was outstanding in his field! But a monkey came along and offered him a banana, so he quit and joined the monkey business instead. (This one's a classic double-entendre for animal lovers!)</s>
[INFO|configuration_utils.py:697] 2025-05-16 12:42:27,252 >> loading configuration file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:771] 2025-05-16 12:42:27,253 >> Model config MistralConfig {
  "_name_or_path": "/data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3979] 2025-05-16 12:42:27,332 >> loading weights file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1633] 2025-05-16 12:42:27,332 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1140] 2025-05-16 12:42:27,335 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.11s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.01it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]
[INFO|modeling_utils.py:4970] 2025-05-16 12:42:30,720 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4978] 2025-05-16 12:42:30,720 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1093] 2025-05-16 12:42:30,853 >> loading configuration file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:1140] 2025-05-16 12:42:30,853 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

[INFO|2025-05-16 12:42:30] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-05-16 12:42:30] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-05-16 12:42:30] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-05-16 12:42:30] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-05-16 12:42:30] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,k_proj,down_proj,up_proj,q_proj,o_proj,v_proj
[INFO|2025-05-16 12:42:31] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:746] 2025-05-16 12:42:31,298 >> Using auto half precision backend
[WARNING|trainer.py:781] 2025-05-16 12:42:31,299 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[INFO|trainer.py:2405] 2025-05-16 12:42:31,669 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-05-16 12:42:31,670 >>   Num examples = 50
[INFO|trainer.py:2407] 2025-05-16 12:42:31,670 >>   Num Epochs = 3
[INFO|trainer.py:2408] 2025-05-16 12:42:31,670 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2411] 2025-05-16 12:42:31,670 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2412] 2025-05-16 12:42:31,670 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2413] 2025-05-16 12:42:31,670 >>   Total optimization steps = 9
[INFO|trainer.py:2414] 2025-05-16 12:42:31,674 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:05<00:44,  5.56s/it] 22%|██▏       | 2/9 [00:10<00:38,  5.45s/it] 33%|███▎      | 3/9 [00:15<00:30,  5.05s/it] 44%|████▍     | 4/9 [00:16<00:16,  3.29s/it] 56%|█████▌    | 5/9 [00:20<00:14,  3.69s/it] 67%|██████▋   | 6/9 [00:25<00:12,  4.00s/it] 78%|███████▊  | 7/9 [00:30<00:09,  4.58s/it] 89%|████████▉ | 8/9 [00:31<00:03,  3.30s/it]100%|██████████| 9/9 [00:36<00:00,  3.90s/it][INFO|trainer.py:3942] 2025-05-16 12:43:08,629 >> Saving model checkpoint to saves/Mistral-7B-Instruct-v0.2/lora/dpo/checkpoint-9
[INFO|configuration_utils.py:697] 2025-05-16 12:43:08,651 >> loading configuration file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:771] 2025-05-16 12:43:08,652 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-05-16 12:43:08,768 >> tokenizer config file saved in saves/Mistral-7B-Instruct-v0.2/lora/dpo/checkpoint-9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-05-16 12:43:08,768 >> Special tokens file saved in saves/Mistral-7B-Instruct-v0.2/lora/dpo/checkpoint-9/special_tokens_map.json
[INFO|trainer.py:2657] 2025-05-16 12:43:08,991 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                             {'train_runtime': 37.3171, 'train_samples_per_second': 4.02, 'train_steps_per_second': 0.241, 'train_loss': 0.5447783999972873, 'epoch': 2.32}
100%|██████████| 9/9 [00:37<00:00,  3.90s/it]100%|██████████| 9/9 [00:37<00:00,  4.11s/it]
[INFO|trainer.py:3942] 2025-05-16 12:43:09,042 >> Saving model checkpoint to saves/Mistral-7B-Instruct-v0.2/lora/dpo
[INFO|configuration_utils.py:697] 2025-05-16 12:43:09,071 >> loading configuration file /data1/zehuali/workspace/zli_work/DSAA6000Q-ass4/model/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:771] 2025-05-16 12:43:09,072 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-05-16 12:43:09,187 >> tokenizer config file saved in saves/Mistral-7B-Instruct-v0.2/lora/dpo/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-05-16 12:43:09,187 >> Special tokens file saved in saves/Mistral-7B-Instruct-v0.2/lora/dpo/special_tokens_map.json
***** train metrics *****
  epoch                    =       2.32
  total_flos               =  4641218GF
  train_loss               =     0.5448
  train_runtime            = 0:00:37.31
  train_samples_per_second =       4.02
  train_steps_per_second   =      0.241
[WARNING|2025-05-16 12:43:09] llamafactory.extras.ploting:148 >> No metric loss to plot.
[WARNING|2025-05-16 12:43:09] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-05-16 12:43:09] llamafactory.extras.ploting:148 >> No metric rewards/accuracies to plot.
[INFO|modelcard.py:449] 2025-05-16 12:43:09,216 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
